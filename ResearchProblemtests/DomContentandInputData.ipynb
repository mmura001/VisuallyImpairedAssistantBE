{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "url = 'https://archive.org/web/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "text_content = soup.get_text()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text_content)\n",
    "for entity in doc.ents:\n",
    "    print(\"Entity:\", entity.text, \"-\", entity.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET THE DOM CONTENT OF THE PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://www.amazon.com/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "dom_structure = soup.prettify()\n",
    "\n",
    "print(dom_structure)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the input fields from the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://archive.org/web/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "input_fields = soup.find_all('input')\n",
    "\n",
    "for input_field in input_fields:\n",
    "    print(\"Input Field:\", input_field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://archive.org/web/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "input_fields = soup.find_all('input')\n",
    "\n",
    "for input_field in input_fields:\n",
    "    print(\"Input Field:\", input_field)\n",
    "\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(extracted_content, json_file)\n",
    "\n",
    "print(\"Data successfully written to\", json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extract_website_info(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            metadata = extract_metadata(soup)\n",
    "            \n",
    "            text_content = extract_text_content(soup)\n",
    "            \n",
    "            links = extract_links(soup)\n",
    "            \n",
    "            images = extract_images(soup)\n",
    "            \n",
    "            tables = extract_tables(soup)\n",
    "            \n",
    "            extracted_info = {\n",
    "                'metadata': metadata,\n",
    "                'text_content': text_content,\n",
    "                'links': links,\n",
    "                'images': images,\n",
    "                'tables': tables\n",
    "            }\n",
    "            \n",
    "            return extracted_info\n",
    "        else:\n",
    "            print(\"Error: Unable to fetch the webpage. Status code:\", response.status_code)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "# Function to extract metadata\n",
    "def extract_metadata(soup):\n",
    "    metadata = {}\n",
    "    title = soup.find('title').get_text() if soup.find('title') else None\n",
    "    metadata['title'] = title\n",
    "    return metadata\n",
    "\n",
    "# Function to extract text content\n",
    "def extract_text_content(soup):\n",
    "    text_content = []\n",
    "    for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):\n",
    "        text_content.append(element.get_text())\n",
    "    return text_content\n",
    "\n",
    "# Function to extract links\n",
    "def extract_links(soup):\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        links.append(link['href'])\n",
    "    return links\n",
    "\n",
    "# Function to extract images\n",
    "def extract_images(soup):\n",
    "    images = []\n",
    "    for img in soup.find_all('img', src=True):\n",
    "        images.append(img['src'])\n",
    "    return images\n",
    "\n",
    "# Function to extract tables\n",
    "def extract_tables(soup):\n",
    "    tables = []\n",
    "    for table in soup.find_all('table'):\n",
    "        table_data = []\n",
    "        for row in table.find_all('tr'):\n",
    "            row_data = [cell.get_text() for cell in row.find_all('td')]\n",
    "            table_data.append(row_data)\n",
    "        tables.append(table_data)\n",
    "    return tables\n",
    "\n",
    "# Example usage\n",
    "url = 'https://archive.org/web/'\n",
    "extracted_info = extract_website_info(url)\n",
    "if extracted_info:\n",
    "    print(\"Metadata:\")\n",
    "    print(extracted_info['metadata'])\n",
    "    print(\"\\nText Content:\")\n",
    "    print(extracted_info['text_content'])\n",
    "    print(\"\\nLinks:\")\n",
    "    print(extracted_info['links'])\n",
    "    print(\"\\nImages:\")\n",
    "    print(extracted_info['images'])\n",
    "    print(\"\\nTables:\")\n",
    "    print(extracted_info['tables'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "\n",
    "\n",
    "url = \"https://archive.org/web/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "text_content = soup.get_text()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text_content)\n",
    "\n",
    "navigation_elements = soup.find_all([\"a\", \"input\"])\n",
    "\n",
    "navigation_data = []\n",
    "entity_association = {}  \n",
    "for entity in doc.ents:\n",
    "    entity_association[entity.text] = []\n",
    "\n",
    "for element in navigation_elements:\n",
    "    if element.name == \"a\": \n",
    "        text = element.get_text().strip()\n",
    "        url = element.get(\"href\", \"\").strip() \n",
    "        navigation_data.append((text, url))\n",
    "    elif element.name == \"input\":  \n",
    "        label = element.get(\"aria-label\", \"\").strip()\n",
    "        name = element.get(\"name\", \"\").strip()  \n",
    "        _id = element.get(\"id\", \"\").strip() \n",
    "        navigation_data.append((\"Input\", label, input_type, name, _id))\n",
    "\n",
    "\n",
    "for entity in doc.ents:\n",
    "    for item in navigation_data:\n",
    "        if len(item) == 5: \n",
    "            if entity.text in item[1]: \n",
    "                entity_association[entity.text].append(item[4]) \n",
    "\n",
    "\n",
    "for item in navigation_data:\n",
    "    if len(item) == 2:\n",
    "        text, url = item\n",
    "        print(\"Text:\", text)\n",
    "        print(\"URL:\", url)\n",
    "    elif len(item) == 5:  \n",
    "  \n",
    "        print(\"Label:\", item[1])\n",
    "        print(\"Type:\", item[2])\n",
    "        print(\"Name:\", item[3])\n",
    "        associated_entities = entity_association.get(item[4], [])  \n",
    "        print(\"Associated Entities:\", associated_entities)\n",
    "    print(\"----------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
